3.1。数据冗余当两个或多个数据样本表示同一实体时，会出现重复。数据重复或不一致对ML的影响可能很严重。尽管在过去20年中发现了一系列用于识别重复的技术[11]，但成对相似性比较等传统方法对大数据来说已不再可行。此外，与非重复对相比，重复对是少数的传统假设不再成立。为此，动态时间扭曲可以比最先进的欧几里德距离算法快得多[12]
3.2。数据噪音
大数据承诺提供来自不同类型的存储库的多视图数据，不同的格式以及不同的人口样本，因此是高度异构的。这些多视图异构数据（例如，非结构化文本，音频和视频格式[15]）可能对于学习任务具有不同的重要性级别。因此，通过将它们视为同等重要来连接所有特征将不可能导致最佳学习结果。大数据提供了从多个视图并行学习的机会，然后通过了解特征视图对任务的重要性来集成多个结果。预计该方法对数据异常值具有鲁棒性，并且可以解决优化难度和收敛问题[16]。
3.4。数据离散化
某些ML算法（如决策树和Na？ve Bayes）只能处理离散属性。离散化将定量数据转换为定性数据，从而获得连续域的非重叠划分。属性离散化的目的是将简洁的数据表示作为类别找到，这足以使学习任务尽可能多地保留原始连续属性中的信息。但是，在处理大数据时，大多数现有的离散化方法都不会有效。为了解决大数据挑战，标准离散化方法通过在大数据平台中基于最小描述长度原理开发分布式版本的熵最小化离散器来实现平衡，提高了性能和准确性[17]。在另一项研究[18]中，数据首先根据数值属性的值进行排序，然后分成原始类属性的片段。这些片段通过不同类别的百分比组成来概括，被视为超级实例和离散化的目标。
3.5。数据标签
传统的数据注释方法是劳动密集型的。已经提出了几种替代方法来解决大数据的挑战。例如，在线人群生成的存储库可以作为免费注释训练数据的来源，可以在类别数量和类内多样性方面捕获大量变化[19]。此外，通过概率程序归纳可以实现人类概念学习[20]。此外，标记数据的能力被内置到ML算法中，例如半监督学习，转移学习和主动学习（例如，[21,22]）。通过使用主动学习作为人群来源数据库中标记任务的优化策略，可以最大限度地减少向人群提出的问题数量，从而允许众包应用程序扩展。然而，为众包数据集设计主动学习算法带来了许多实际挑战，例如通用性，可扩展性和易用性[23]。另一个问题是，这样的数据集可能无法覆盖所有用户特定的上下文，这可能导致性能明显差于以用户为中心的培训[19]。
3.6。不平衡的数据
传统的分层随机抽样方法已经解决了数据不平衡的问题。但是，如果涉及子样本生成和错误度量计算的迭代，则该过程可能非常耗时。此外，传统的采样方法无法有效地支持用户指定的数据子集（包括基于值的采样）的数据采样。大数据需要并行数据采样。例如，已经提出了一种并行采样框架，用于基于多个分布式索引文件从原始数据集中生成样本数据集[24]。可以基于数据集大小和可用过程来选择并行级别
3.7。特征表示和选择
ML的性能在很大程度上取决于数据表示或特征的选择[6]。 ML算法的普遍性取决于数据集，数据集也间接依赖于表示数据集的显着结构的特征。通过识别突出特征，特征选择有助于提高ML的性能。它基本上选择不同的特征和数据子集，并以不同的粒度级别聚合它们，这有助于减少大数据量。然而，特征工程需要先前的领域知识和人类的聪明才智，并且通常是劳动密集型的[6]。为了解决当前特征工程算法在处理大数据时的弱点，已经提出了各种解决方案，例如分布式特征选择[25];低秩矩阵近似（例如，标准Nystr？m方法[26]）;表示学习通过学习一般先验[6]使学习算法减少对特征工程的依赖;用于超高维特征选择的自适应特征缩放方案，其迭代地激活一组特征并解决多个核学习子问题的序列[27];基于谱图理论的统一的特征选择框架，能够为监督和非监督特征选择生成算法族[28];分类前的模糊聚类，其中分组以群组为中心实现，然后通过减少数据进行去聚类和分类[29];并且减小了数据维度和体积的大小（例如，随机森林前向选择排名和随机森林后退消除排名[30]，以及具有所选特征的语言对冲神经模糊分类器[31]）。最近，基于深度神经网络的自动编码已被证明在学习视频，音频和文本功能方面非常有效[32,33]
