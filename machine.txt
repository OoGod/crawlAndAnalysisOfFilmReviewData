Much of the actual effort in deploying an ML system goes into the
design of preprocessing pipelines and data transformations that result
in a representation of data that can support effective ML [6]. Data
preprocessing aims to address a number of issues such as data
redundancy, inconsistency, noise, heterogeneity, transformation, labeling (for (semi-)supervised ML), data imbalance and feature representation/selection. Data preparation and preprocessing is usually costly,
due to the requirement of human labor and a large number of options
to choose from. Additionally, some conventional data assumptions do
not hold for big data, consequently some preprocessing methods
become infeasible. On the other hand, big data creates the opportunity
of reducing the reliance on human supervision by learning from
massive, diverse, and streaming data sources directly